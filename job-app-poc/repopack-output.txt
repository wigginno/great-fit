This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-04-10T20:16:03.404Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
static/
  index.html
  script.js
  style.css
.gitignore
conftest.py
crud.py
database.py
llm_interaction.py
logic.py
main.py
models.py
pytest.ini
requirements.txt
schemas.py
tampermonkey_script.js
test_form.html
test_main.py

================================================================
Repository Files
================================================================

================
File: static/index.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Job Application Helper</title>
    <link rel="stylesheet" href="/static/style.css">
</head>
<body>
    <div class="container">
        <h1>Job Application Helper</h1>
        
        <div class="section">
            <h2>Job Description</h2>
            <textarea id="jobDescription" placeholder="Paste job description here..."></textarea>
            <button id="saveJobButton" class="btn">Save Job</button>
        </div>

        <div class="section">
            <h2>User Profile</h2>
            <button id="loadProfileButton" class="btn">Load Profile</button>
            <div id="userProfile" class="content-display">
                <!-- Profile content will be loaded here -->
            </div>
        </div>

        <div class="section">
            <h2>Saved Jobs</h2>
            <button id="loadJobsButton" class="btn">Load Jobs</button>
            <div id="jobsList" class="content-display">
                <!-- Jobs list will be displayed here -->
            </div>
            <div id="jobDetails" class="content-display">
                <!-- Job details will be displayed here when a job is selected -->
            </div>
        </div>

        <div class="section">
            <h2>Tailoring Suggestions</h2>
            <button id="tailorButton" class="btn">Get Tailoring Suggestions</button>
            <div id="tailoringSuggestions" class="content-display">
                <!-- Tailoring suggestions will appear here -->
            </div>
        </div>
    </div>

    <script src="/static/script.js"></script>
</body>
</html>

================
File: static/script.js
================
// --- Core Functions --- //

// Function to save a job description
async function saveJob() {
    const jobDescription = document.getElementById('jobDescription').value;
    if (!jobDescription.trim()) {
        alert('Please enter a job description first');
        return;
    }

    // For now, hardcode user ID to 1
    const userId = 1;

    try {
        const response = await fetch(`/users/${userId}/jobs`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                description: jobDescription,
                status: 'New', // Default status
                title: 'Job from Website', // Generic title
            }),
        });

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.detail || `HTTP error! status: ${response.status}`);
        }

        const savedJob = await response.json();
        alert('Job saved successfully!');
        
        // Clear the textarea or keep the text based on UX preference
        // document.getElementById('jobDescription').value = '';
        
        // Optionally refresh the jobs list
        loadJobs();
    } catch (error) {
        console.error('Error saving job:', error);
        alert(`Failed to save job: ${error.message}`);
    }
}

// Function to load and display user profile
async function loadProfile() {
    try {
        // For now, hardcode user ID to 1
        const userId = 1;
        const profileContainer = document.getElementById('userProfile');
        profileContainer.innerHTML = 'Loading profile...';

        const response = await fetch(`/users/${userId}/profile`);
        
        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.detail || `HTTP error! status: ${response.status}`);
        }

        const profile = await response.json();
        
        // Display the profile content
        profileContainer.innerHTML = `
            <h3>${profile.user_id ? 'User ' + profile.user_id : 'Profile'}</h3>
            <pre>${profile.content}</pre>
        `;
        
        // Optionally store the user ID in a data attribute for later use
        profileContainer.setAttribute('data-user-id', profile.user_id);
        
    } catch (error) {
        console.error('Error loading profile:', error);
        document.getElementById('userProfile').innerHTML = 
            `<p style="color: red;">Error loading profile: ${error.message}</p>`;
    }
}

// Function to load and display all saved jobs
async function loadJobs() {
    try {
        // For now, hardcode user ID to 1
        const userId = 1;
        const jobsListContainer = document.getElementById('jobsList');
        jobsListContainer.innerHTML = 'Loading jobs...';

        const response = await fetch(`/users/${userId}/jobs`);
        
        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.detail || `HTTP error! status: ${response.status}`);
        }

        const jobs = await response.json();
        
        if (jobs.length === 0) {
            jobsListContainer.innerHTML = '<p>No jobs found. Save a job description to get started.</p>';
            return;
        }
        
        // Create a list of jobs
        let jobsList = '<ul class="jobs-list">';
        jobs.forEach(job => {
            // Add a class based on the job's ranking score if available
            const scoreClass = job.ranking_score ? 
                (job.ranking_score >= 7 ? 'high-match' : 
                 job.ranking_score >= 5 ? 'medium-match' : 'low-match') : '';
                 
            jobsList += `
                <li class="job-item ${scoreClass}" data-job-id="${job.id}">
                    <span class="job-title">${job.title || 'Untitled Job'}</span>
                    ${job.ranking_score ? `<span class="job-score">Score: ${job.ranking_score}</span>` : ''}
                </li>
            `;
        });
        jobsList += '</ul>';
        
        jobsListContainer.innerHTML = jobsList;
        
        // Add click event listeners to job items
        document.querySelectorAll('.job-item').forEach(jobItem => {
            jobItem.addEventListener('click', async () => {
                // Get job details and show them
                const jobId = jobItem.getAttribute('data-job-id');
                await showJobDetails(jobId, userId);
                
                // Update active state
                document.querySelectorAll('.job-item').forEach(item => {
                    item.classList.remove('active');
                });
                jobItem.classList.add('active');
            });
        });
        
    } catch (error) {
        console.error('Error loading jobs:', error);
        document.getElementById('jobsList').innerHTML = 
            `<p style="color: red;">Error loading jobs: ${error.message}</p>`;
    }
}

// Function to show job details and add ranking functionality
async function showJobDetails(jobId, userId) {
    try {
        const jobDetailsContainer = document.getElementById('jobDetails');
        jobDetailsContainer.innerHTML = 'Loading job details...';

        const response = await fetch(`/users/${userId}/jobs/${jobId}`);
        
        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.detail || `HTTP error! status: ${response.status}`);
        }

        const job = await response.json();
        
        // Create the job details HTML with a button to rank the job
        let jobDetailsHTML = `
            <h3>${job.title || 'Job Details'}</h3>
            <div class="job-description">
                <h4>Description:</h4>
                <pre>${job.description}</pre>
            </div>
        `;
        
        // Add ranking information if available
        if (job.ranking_score) {
            jobDetailsHTML += `
                <div class="job-ranking">
                    <h4>Match Score: ${job.ranking_score}/10</h4>
                    <p>${job.ranking_explanation || ''}</p>
                </div>
            `;
        }
        
        // Add a button to rank/re-rank the job
        jobDetailsHTML += `
            <button id="rankJobButton" class="btn" data-job-id="${job.id}">
                ${job.ranking_score ? 'Re-rank Job' : 'Rank Job'}
            </button>
        `;
        
        jobDetailsContainer.innerHTML = jobDetailsHTML;
        
        // Add event listener to rank button
        document.getElementById('rankJobButton').addEventListener('click', async () => {
            await rankJob(job.id, userId);
        });
        
    } catch (error) {
        console.error('Error showing job details:', error);
        document.getElementById('jobDetails').innerHTML = 
            `<p style="color: red;">Error loading job details: ${error.message}</p>`;
    }
}

// Function to rank a job against the user profile
async function rankJob(jobId, userId) {
    try {
        const rankButton = document.getElementById('rankJobButton');
        const jobDetailsContainer = document.getElementById('jobDetails');
        
        // Update UI to show ranking in progress
        rankButton.disabled = true;
        rankButton.textContent = 'Ranking...';
        
        // Add a message in the job details about ranking in progress
        jobDetailsContainer.innerHTML += '<p id="ranking-status">Ranking job against your profile. This may take a moment...</p>';
        
        // Call the ranking endpoint
        const response = await fetch(`/rank-job/${jobId}`, {
            method: 'POST',
        });
        
        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.detail || `HTTP error! status: ${response.status}`);
        }

        const result = await response.json();
        
        // Refresh the job details to show the new ranking
        await showJobDetails(jobId, userId);
        
        // Also refresh the jobs list to update the score display
        await loadJobs();
        
    } catch (error) {
        console.error('Error ranking job:', error);
        document.getElementById('ranking-status').innerHTML = 
            `<p style="color: red;">Error ranking job: ${error.message}</p>`;
        
        // Re-enable the button
        const rankButton = document.getElementById('rankJobButton');
        if (rankButton) {
            rankButton.disabled = false;
            rankButton.textContent = 'Try Ranking Again';
        }
    }
}

// --- Tailoring Suggestions --- //

// Function to fetch tailoring suggestions
async function fetchTailoringSuggestions() {
    const jobDescription = document.getElementById('jobDescription').value;
    // Assuming user profile data is stored or accessible, e.g., from the profile display area
    // Adjust this selector if your profile display structure is different
    const userProfileElement = document.getElementById('userProfile');
    const userProfileText = userProfileElement ? userProfileElement.textContent || userProfileElement.innerText : ''; // Basic text extraction
    const suggestionsContainer = document.getElementById('tailoringSuggestions');
    suggestionsContainer.innerHTML = 'Loading tailoring suggestions...'; // Provide feedback

    if (!jobDescription) {
        suggestionsContainer.innerHTML = '<p style="color: red;">Please enter a job description first.</p>';
        return;
    }

    // We need the user profile ID to fetch the full profile on the backend
    // Let's assume the user ID is stored somewhere accessible, e.g., a data attribute after loading the profile
    // For now, let's hardcode user ID 1, but this needs a proper implementation
    const userId = 1; // Placeholder - this needs to be dynamically set after profile load

    if (!userId) {
         suggestionsContainer.innerHTML = '<p style="color: red;">Please load a user profile first.</p>';
         return;
    }


    try {
        const response = await fetch(`/users/${userId}/jobs/tailor`, { // Endpoint needs to be created
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({ job_description: jobDescription }), // Sending only job description for now
                                                                      // Backend can fetch profile using userId
        });

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.detail || `HTTP error! status: ${response.status}`);
        }

        const data = await response.json();
        // Assuming the response has a 'suggestions' field
        suggestionsContainer.innerHTML = `<h3>Tailoring Suggestions:</h3><p>${data.suggestions.replace(/\n/g, '<br>')}</p>`;

    } catch (error) {
        console.error('Error fetching tailoring suggestions:', error);
        suggestionsContainer.innerHTML = `<p style="color: red;">Error fetching suggestions: ${error.message}</p>`;
    }
}

// Add event listeners when the DOM is loaded
document.addEventListener('DOMContentLoaded', () => {
    const saveJobButton = document.getElementById('saveJobButton');
    const loadProfileButton = document.getElementById('loadProfileButton');
    const loadJobsButton = document.getElementById('loadJobsButton');
    const tailorButton = document.getElementById('tailorButton'); // Get the tailor button

    if (saveJobButton) {
        saveJobButton.addEventListener('click', saveJob);
    } else {
        console.warn('Save Job button (id="saveJobButton") not found.');
    }

    if (loadProfileButton) {
        loadProfileButton.addEventListener('click', loadProfile);
    } else {
        console.warn('Load Profile button (id="loadProfileButton") not found.');
    }

    if (loadJobsButton) {
        loadJobsButton.addEventListener('click', loadJobs);
    } else {
        console.warn('Load Jobs button (id="loadJobsButton") not found.');
    }

    // Attach event listener for the tailor button
    if (tailorButton) {
        tailorButton.addEventListener('click', fetchTailoringSuggestions);
    } else {
        console.warn('Tailor button (id="tailorButton") not found.');
    }

    // Initial load (optional)
    // loadProfile();
    // loadJobs();
});

================
File: static/style.css
================
body {
    font-family: sans-serif;
    margin: 20px;
    background-color: #f4f4f4;
}

h1 {
    text-align: center;
    color: #333;
}

.container {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 20px;
    max-width: 1200px;
    margin: 20px auto;
    background-color: #fff;
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 0 10px rgba(0,0,0,0.1);
}

section {
    border: 1px solid #ddd;
    padding: 15px;
    border-radius: 5px;
    background-color: #fff;
}

h2 {
    margin-top: 0;
    color: #555;
    border-bottom: 1px solid #eee;
    padding-bottom: 5px;
}

textarea {
    width: 95%;
    margin-bottom: 10px;
    padding: 8px;
    border: 1px solid #ccc;
    border-radius: 4px;
}

button {
    padding: 10px 15px;
    background-color: #007bff;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    margin-right: 5px;
    margin-bottom: 5px;
}

button:hover {
    background-color: #0056b3;
}

pre {
    background-color: #eee;
    padding: 10px;
    border-radius: 4px;
    white-space: pre-wrap; /* Allows wrapping */
    word-wrap: break-word; /* Breaks long words */
    max-height: 300px;
    overflow-y: auto;
}

#jobs-list div {
    border: 1px solid #eee;
    padding: 10px;
    margin-bottom: 5px;
    cursor: pointer;
    border-radius: 4px;
}

#jobs-list div:hover {
    background-color: #f0f0f0;
}

#job-details-display,
#tailoring-suggestions {
    margin-top: 10px;
    background-color: #f9f9f9;
    padding: 10px;
    border: 1px solid #eee;
    border-radius: 4px;
    min-height: 50px;
}

input[type="text"] {
    padding: 8px;
    margin-right: 5px;
    border: 1px solid #ccc;
    border-radius: 4px;
}

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

================
File: conftest.py
================
import pytest
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from fastapi.testclient import TestClient

# Import app and DB dependency function first
from main import app, get_db
# Import database components needed for setup
from database import Base

# Fixed test database URL for consistency
TEST_DATABASE_URL = "sqlite:///./test_job_assistant_poc.db"

# Create a test engine and session factory
connect_args = {"check_same_thread": False} if TEST_DATABASE_URL.startswith("sqlite") else {}
test_engine = create_engine(TEST_DATABASE_URL, connect_args=connect_args)
TestSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=test_engine)

@pytest.fixture(scope="session", autouse=True)
def setup_test_database():
    """Create the test database once for the entire test session."""
    # Delete old db file if it exists
    db_path = TEST_DATABASE_URL.split("///")[-1]
    if os.path.exists(db_path):
        try:
            os.unlink(db_path)
            print(f"\nRemoved existing test database file: {db_path}")
        except OSError as e:
            print(f"Error removing existing test database file {db_path}: {e}")
    
    # Create all tables
    print(f"Creating test database tables at {db_path}")
    Base.metadata.create_all(bind=test_engine)
    
    yield  # Tests run here
    
    # Clean up after all tests complete
    test_engine.dispose()
    if os.path.exists(db_path):
        try:
            os.unlink(db_path)
            print(f"Removed test database file: {db_path}")
        except OSError as e:
            print(f"Error removing test database file {db_path}: {e}")


@pytest.fixture(scope="function")
def db_session():
    """Provides a database session for each test function.
    
    This session will automatically commit when db.commit() is called, which
    ensures data created in one test is visible to subsequent API calls.
    """
    session = TestSessionLocal()
    try:
        yield session
    finally:
        session.close()

# Override the get_db dependency for tests
@pytest.fixture(scope="function")
def override_get_db():
    """Override the get_db dependency to use our test database.
    
    This creates a new session for each API call, allowing proper
    transaction handling within FastAPI endpoints.
    """
    def _override_get_db():
        db = TestSessionLocal()
        try:
            yield db
        finally:
            db.close()
    
    # Set the override
    original = app.dependency_overrides.get(get_db)
    app.dependency_overrides[get_db] = _override_get_db
    
    yield
    
    # Restore original after test
    if original:
        app.dependency_overrides[get_db] = original
    else:
        del app.dependency_overrides[get_db]


@pytest.fixture(scope="function")
def test_client(override_get_db):
    """Provides a test client configured with our test database session."""
    return TestClient(app)

================
File: crud.py
================
import json
from sqlalchemy.orm import Session

import models
import schemas

# --- User CRUD ---
def get_user_by_email(db: Session, email: str):
    return db.query(models.User).filter(models.User.email == email).first()

def create_user(db: Session, user: schemas.UserCreate):
    # In a real app, hash the password here
    db_user = models.User(email=user.email)
    db.add(db_user)
    db.flush() # Assign ID without committing
    db.refresh(db_user)
    return db_user

# --- User Profile CRUD ---
def get_user_profile(db: Session, user_id: int):
    user = db.query(models.User).filter(models.User.id == user_id).first()
    if user:
        return user.profile_json # Returns the raw JSON string
    return None

def create_or_update_user_profile(db: Session, user_id: int, profile: schemas.UserProfileCreate):
    user = db.query(models.User).filter(models.User.id == user_id).first()
    if not user:
        return None # Or raise HTTPException in the endpoint

    profile_json_string = json.dumps(profile.profile_data)
    user.profile_json = profile_json_string
    db.add(user) # add works for updates too
    db.flush() # Ensure changes are sent before refresh
    db.refresh(user)
    # Return the stored JSON string, endpoint will handle parsing for response model
    return user.profile_json

# --- Job CRUD ---
def create_job(db: Session, job: schemas.JobCreate, user_id: int):
    db_job = models.Job(**job.model_dump(), user_id=user_id)
    db.add(db_job)
    db.flush()
    db.refresh(db_job)
    return db_job

def get_jobs_for_user(db: Session, user_id: int):
    return db.query(models.Job).filter(models.Job.user_id == user_id).all()

def get_job(db: Session, job_id: int, user_id: int):
    return db.query(models.Job).filter(models.Job.id == job_id, models.Job.user_id == user_id).first()

def update_job_ranking(db: Session, job_id: int, user_id: int, score: float, explanation: str):
    db_job = db.query(models.Job).filter(models.Job.id == job_id, models.Job.user_id == user_id).first()
    if not db_job:
        return None # Or raise HTTPException in the endpoint

    db_job.ranking_score = score
    db_job.ranking_explanation = explanation
    db.add(db_job)
    db.flush()
    db.refresh(db_job)
    return db_job

================
File: database.py
================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

SQLALCHEMY_DATABASE_URL = "sqlite:///./job_assistant_poc.db"
# SQLALCHEMY_DATABASE_URL = "postgresql://user:password@postgresserver/db"

engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False} # Needed only for SQLite
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def create_db_and_tables():
    # NOTE: This is a simple way to create tables for a PoC.
    # In a real application, you would likely use Alembic migrations.
    Base.metadata.create_all(bind=engine)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

================
File: llm_interaction.py
================
import google.generativeai as genai
import os
import dotenv
import json
import logging
import asyncio # Needed for async sleep in retry

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables from .env file
dotenv.load_dotenv()

# --- Configure Gemini API ---
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    logger.error("GEMINI_API_KEY not found in environment variables.")
    # Depending on the application structure, you might raise an error here
    # or handle it such that API calls will gracefully fail.
    genai.configure(api_key="DUMMY_KEY_FOR_INITIALIZATION") # Avoid crashing if key missing
else:
    try:
        genai.configure(api_key=API_KEY)
    except Exception as e:
        logger.error(f"Failed to configure Gemini API: {e}")
        # Handle configuration error appropriately

# --- Generation Configuration ---
# Adjust temperature for creativity vs. predictability (0.0-1.0)
# For JSON output, lower temperature might be more reliable.
DEFAULT_GENERATION_CONFIG = genai.types.GenerationConfig(
    temperature=0.5, # Lower temperature for more predictable JSON
    # max_output_tokens=2048, # Optional: limit response size
    # top_p=1.0, # Optional
    # top_k=1  # Optional
)

JSON_GENERATION_CONFIG = genai.types.GenerationConfig(
    response_mime_type="application/json",
    temperature=0.2 # Even lower temperature for stricter JSON adherence
)

# --- Safety Settings ---
# WARNING: Blocking none is permissive. Review for production use.
# Consider adjusting HARM_CATEGORY settings based on application needs.
SAFETY_SETTINGS = {
    genai.types.HarmCategory.HARM_CATEGORY_HARASSMENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
    genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: genai.types.HarmBlockThreshold.BLOCK_NONE,
    genai.types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: genai.types.HarmBlockThreshold.BLOCK_NONE,
    genai.types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: genai.types.HarmBlockThreshold.BLOCK_NONE,
}

# --- Gemini Model Interaction ---
# Use the recommended model, e.g., 'gemini-1.5-pro-latest' or 'gemini-1.5-flash-latest'
MODEL_NAME = "gemini-1.5-pro-latest"

async def call_gemini(prompt: str, expect_json: bool = False, retries: int = 3, delay: int = 5):
    """Calls the Gemini API asynchronously with retries.

    Args:
        prompt: The text prompt to send to the model.
        expect_json: If True, configure the model for JSON output.
        retries: Number of retry attempts on failure.
        delay: Delay in seconds between retries.

    Returns:
        The response text from the model, or None if an error occurs after retries.
        If expect_json is True, attempts to parse the response as JSON.
        Returns the parsed dict/list or None if parsing fails.
    """
    if not API_KEY or API_KEY == "DUMMY_KEY_FOR_INITIALIZATION":
        logger.error("Gemini API key not configured. Cannot make API call.")
        return None

    generation_config = JSON_GENERATION_CONFIG if expect_json else DEFAULT_GENERATION_CONFIG

    model = genai.GenerativeModel(
        MODEL_NAME,
        generation_config=generation_config,
        safety_settings=SAFETY_SETTINGS
    )

    for attempt in range(retries):
        try:
            logger.info(f"Calling Gemini API (Attempt {attempt + 1}/{retries}). Expect JSON: {expect_json}")
            response = await model.generate_content_async(prompt)

            if not response.parts:
                 logger.warning("Gemini response has no parts.")
                 # Handle cases where the response might be blocked or empty
                 # Check response.prompt_feedback for block reasons
                 if response.prompt_feedback:
                     logger.warning(f"Prompt Feedback: {response.prompt_feedback}")
                 return None # Or handle differently based on feedback

            response_text = response.text
            logger.info("Gemini API call successful.")

            if expect_json:
                try:
                    return json.loads(response_text)
                except json.JSONDecodeError as json_e:
                    logger.error(f"Failed to decode Gemini response as JSON: {json_e}")
                    logger.debug(f"Raw response: {response_text}")
                    # Fall through to retry logic or return None after last attempt
                    if attempt == retries - 1:
                        return None
                    # Optional: Consider not retrying on JSON decode error?
            else:
                return response_text # Return raw text if JSON not expected

        except Exception as e:
            logger.error(f"Error calling Gemini API (Attempt {attempt + 1}/{retries}): {e}")
            if attempt == retries - 1:
                logger.error("Max retries reached. Returning None.")
                return None
            logger.info(f"Retrying in {delay} seconds...")
            await asyncio.sleep(delay)

    return None # Should theoretically not be reached, but ensures a return path

# Example usage (can be run standalone for testing if needed)
async def main_test():
    test_prompt = "Write a short story about a robot learning to paint."
    print(f"Testing with prompt: {test_prompt}")
    response = await call_gemini(test_prompt)
    if response:
        print("\n--- Response Text ---")
        print(response)
    else:
        print("\nFailed to get response.")

    test_json_prompt = "Create a JSON object with two keys: 'name' (string) and 'age' (integer). Use 'Bob' and 30."
    print(f"\nTesting with JSON prompt: {test_json_prompt}")
    json_response = await call_gemini(test_json_prompt, expect_json=True)
    if json_response:
        print("\n--- Response JSON ---")
        print(json.dumps(json_response, indent=2))
    else:
        print("\nFailed to get JSON response.")

if __name__ == "__main__":
    asyncio.run(main_test())

================
File: logic.py
================
import logging
import re
import json
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional

# Project imports
from llm_interaction import call_gemini
import crud
import models
import schemas

logger = logging.getLogger(__name__)

# --- Helper for Nested Dict Lookup ---
def get_value_from_nested_dict(data_dict: Dict[str, Any], key_string: str) -> Optional[Any]:
    """Retrieves a value from a nested dictionary using a dot-separated key string."""
    keys = key_string.split('.')
    value = data_dict
    try:
        for key in keys:
            if isinstance(value, list):
                try:
                    index = int(key)
                    if 0 <= index < len(value):
                        value = value[index]
                    else:
                        logger.debug(f"Index {index} out of bounds for key '{key_string}'")
                        return None # Index out of bounds
                except ValueError:
                    logger.debug(f"Key '{key}' is not a valid list index for key '{key_string}'")
                    return None # Key is not a valid list index
            elif isinstance(value, dict):
                value = value[key]
            else:
                logger.debug(f"Cannot traverse further at key '{key}' for key '{key_string}'")
                return None # Cannot traverse further (e.g., trying to key into a string)
        return value
    except (KeyError, TypeError, IndexError) as e:
        logger.debug(f"Error accessing key '{key_string}': {e}")
        return None # Key not found, invalid type for access, or index error

# --- Job Ranking Logic ---
async def rank_job_with_llm(db: Session, job_id: int, user_id: int):
    """Ranks a job based on user profile using LLM and updates the DB.

    Args:
        db: SQLAlchemy session.
        job_id: ID of the job to rank.
        user_id: ID of the user whose profile to use.

    Returns:
        A tuple (score, explanation) or (None, None) if an error occurs.
    """
    logger.info(f"Ranking job {job_id} for user {user_id}")

    # 1. Fetch data
    db_job = crud.get_job(db, job_id=job_id, user_id=user_id)
    if not db_job:
        logger.error(f"Job {job_id} not found for user {user_id}")
        return None, None

    profile_json_string = crud.get_user_profile(db, user_id=user_id)
    if not profile_json_string:
        logger.warning(f"User profile not found for user {user_id}. Ranking based on job only.")
        # Optional: Decide if you want to proceed without a profile or return an error.
        # For now, we'll proceed but the prompt quality might suffer.
        profile_snippet = "User profile not available."
    else:
        # Keep the profile snippet concise for the prompt
        try:
            profile_data = json.loads(profile_json_string)
            # Extract relevant parts, e.g., summary, skills, recent experience
            # This is a placeholder - adjust based on actual profile structure
            profile_snippet = json.dumps({
                "summary": profile_data.get("summary", "N/A"),
                "skills": profile_data.get("skills", [])[:5], # Limit skills shown
            })
        except json.JSONDecodeError:
             logger.error(f"Failed to parse profile JSON for user {user_id}")
             profile_snippet = "Error parsing profile."
        except Exception as e:
             logger.error(f"Error processing profile for user {user_id}: {e}")
             profile_snippet = "Error processing profile."


    job_description_text = db_job.description_text

    # 2. Construct prompt
    prompt = f"""Analyze the following job description and user profile snippet.
Provide a relevance score from 1 (low) to 10 (high) and a brief one-sentence explanation.

Job Description:
```
{job_description_text}
```

User Profile Snippet:
```json
{profile_snippet}
```

Respond ONLY in the format:
Score: [score]
Explanation: [explanation]"""

    # 3. Call LLM
    llm_response = await call_gemini(prompt)

    if not llm_response:
        logger.error(f"LLM call failed for job {job_id}, user {user_id}")
        return None, None

    # 4. Parse response
    try:
        score_match = re.search(r"Score:\s*([\d\.]+)", llm_response)
        explanation_match = re.search(r"Explanation:\s*(.*)", llm_response, re.IGNORECASE)

        if not score_match or not explanation_match:
            logger.error(f"Could not parse LLM response for job {job_id}. Response: {llm_response}")
            return None, None

        score_str = score_match.group(1)
        explanation = explanation_match.group(1).strip()
        score = float(score_str)

        # Clamp score just in case
        score = max(1.0, min(10.0, score))

    except (ValueError, AttributeError) as e:
        logger.error(f"Error parsing score/explanation from LLM response for job {job_id}: {e}. Response: {llm_response}")
        return None, None

    # 5. Update DB
    updated_job = crud.update_job_ranking(db, job_id=job_id, user_id=user_id, score=score, explanation=explanation)
    if not updated_job:
        logger.error(f"Failed to update job ranking in DB for job {job_id}")
        # Score was generated but not saved, potential inconsistency
        return None, None # Indicate failure to save

    logger.info(f"Successfully ranked job {job_id} for user {user_id}. Score: {score}")
    return score, explanation

# --- Resume Tailoring Logic ---
async def suggest_resume_tailoring(job_description: str, profile_snippet: str):
    """Generates resume tailoring suggestions using LLM.

    Args:
        job_description: The text of the job description.
        profile_snippet: A relevant snippet from the user's profile/resume.

    Returns:
        A string containing tailoring suggestions, or None if an error occurs.
    """
    logger.info("Generating resume tailoring suggestions.")
    prompt = f"""Given the following job description and a snippet from a user's profile/resume, provide 3-5 specific, actionable suggestions (as bullet points) on how to tailor the profile snippet to better match the job description.
Focus on incorporating keywords, highlighting relevant skills/experience, and using quantifiable achievements where possible.

Job Description:
```
{job_description}
```

Profile Snippet:
```
{profile_snippet}
```

Suggestions:"""

    llm_response = await call_gemini(prompt)

    if not llm_response:
        logger.error("LLM call failed for resume tailoring suggestions.")
        return None

    logger.info("Successfully generated resume tailoring suggestions.")
    return llm_response.strip() # Return the raw suggestions

# --- Autofill Mapping Logic ---
async def map_form_fields_with_llm(db: Session, user_id: int, form_fields: List[schemas.FormFieldInfo]) -> Dict[str, str]:
    """Maps user profile data to form fields using LLM.

    Args:
        db: SQLAlchemy session.
        user_id: ID of the user whose profile to use.
        form_fields: A list of FormFieldInfo objects representing the form.

    Returns:
        A dictionary mapping field_id to the corresponding string value from the user profile.
    """
    logger.info(f"Mapping form fields for user {user_id}")

    # 1. Fetch and parse user profile
    profile_json_string = crud.get_user_profile(db, user_id=user_id)
    if not profile_json_string:
        logger.error(f"User profile not found for user {user_id}. Cannot perform mapping.")
        return {}

    try:
        user_profile_dict = json.loads(profile_json_string)
        # The actual profile data might be nested under 'profile_data'
        if 'profile_data' in user_profile_dict and isinstance(user_profile_dict['profile_data'], dict):
            user_profile_dict = user_profile_dict['profile_data']
        logger.debug(f"User profile data for mapping: {user_profile_dict}")
    except json.JSONDecodeError:
        logger.error(f"Failed to parse profile JSON for user {user_id}. Cannot perform mapping.")
        return {}

    # 2. Prepare form fields data for prompt
    form_fields_list = [field.model_dump(exclude_none=True) for field in form_fields]
    try:
        form_fields_json = json.dumps(form_fields_list, indent=2)
    except TypeError:
         logger.error("Failed to serialize form fields to JSON.")
         return {}

    # 3. Construct prompt for LLM (JSON mode)
    prompt = f"""You are an expert form-filling assistant. Analyze the provided web form fields and user profile data. Map the user data to the appropriate form fields based on semantic meaning (labels, names, types, placeholders).

User Profile Data:
```json
{json.dumps(user_profile_dict, indent=2)}
```

Form Fields:
```json
{form_fields_json}
```

Task: Return a JSON object mapping the `field_id` from the Form Fields list to the corresponding **full path key** from the User Profile Data JSON (e.g., 'contact.firstName', 'experience.0.company', 'skills.2'). If a form field cannot be confidently mapped to a specific profile key, omit its `field_id` from the result JSON object. Respond ONLY with the JSON mapping object.

Example Response Format:
{{'field_id_for_firstname': 'contact.firstName', 'field_id_for_email': 'contact.email', 'field_id_for_company': 'experience.0.company'}}
"""

    # 4. Call LLM (expecting JSON mapping field_id -> profile_key)
    logger.info("Calling LLM for form field mapping...")
    llm_key_mapping = await call_gemini(prompt, expect_json=True)

    if not llm_key_mapping or not isinstance(llm_key_mapping, dict):
        logger.error(f"LLM did not return a valid JSON dictionary for field mapping. Response: {llm_key_mapping}")
        return {}

    logger.info(f"LLM returned key mapping: {llm_key_mapping}")

    # 5. Look up actual values using the keys returned by LLM
    final_mapping: Dict[str, str] = {}
    for field_id, profile_key in llm_key_mapping.items():
        if not isinstance(profile_key, str):
            logger.warning(f"LLM returned non-string key '{profile_key}' for field '{field_id}'. Skipping.")
            continue

        actual_value = get_value_from_nested_dict(user_profile_dict, profile_key)

        if actual_value is not None:
            # Convert non-string values (like numbers, bools) to string for form filling
            final_mapping[field_id] = str(actual_value)
            logger.debug(f"Mapped field '{field_id}' -> key '{profile_key}' -> value '{str(actual_value)}'")
        else:
            logger.warning(f"LLM mapped field '{field_id}' to key '{profile_key}', but value not found/accessible in profile.")

    logger.info(f"Final autofill mapping generated: {final_mapping}")
    return final_mapping

# --- New Function: Tailoring Suggestions --- #

async def get_tailoring_suggestions(profile_text: str, job_description: str) -> str:
    """
    Generates tailoring suggestions using Gemini based on user profile and job description.

    Args:
        profile_text: The content of the user's profile/resume.
        job_description: The text of the job description.

    Returns:
        A string containing tailoring suggestions.
    """
    print("Generating tailoring suggestions via Gemini...") # Log start

    prompt = f"""
    Given the following user profile/resume and job description, provide specific, actionable suggestions
    on how the user can tailor their profile/resume to better match the requirements and keywords
    in the job description. Focus on highlighting relevant skills, experiences, and keywords.
    Format the suggestions as a clear, concise list or paragraph.

    User Profile/Resume:
    ---
    {profile_text}
    ---

    Job Description:
    ---
    {job_description}
    ---

    Tailoring Suggestions:
    """

    try:
        # Use call_gemini instead of OpenAI client
        suggestions = await call_gemini(prompt, is_json_response=False) # Assuming suggestions are plain text

        print("Successfully generated suggestions via Gemini.") # Log success
        return suggestions

    except Exception as e:
        print(f"Error calling Gemini for tailoring suggestions: {e}")
        # Re-raise the exception so the endpoint can handle it
        raise Exception(f"LLM (Gemini) API call failed: {e}")

================
File: main.py
================
from fastapi import FastAPI, Depends, HTTPException, Request, Path
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from sqlalchemy.orm import Session
from typing import List, Dict
import json
from starlette.requests import Request
from pydantic import BaseModel
import os

import models, schemas, crud, logic
from database import SessionLocal, engine, create_db_and_tables, get_db

# Create DB tables on startup
# In a real app, use Alembic migrations
create_db_and_tables()

app = FastAPI(
    title="Job Application Assistant PoC",
    description="Backend API for the Job Application Assistant PoC",
    version="0.1.0"
)

# --- CORS Middleware --- Set up CORS
# Allow all origins for PoC purposes. Restrict in production.
origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"], # Allows all methods
    allow_headers=["*"], # Allows all headers
)

# Mount static files directory
app.mount("/static", StaticFiles(directory="static"), name="static")

# --- Root Endpoint --- Serve index.html using FileResponse
@app.get("/", response_class=FileResponse)
async def read_root():
    # Check if file exists before returning, optional but good practice
    index_path = "static/index.html"
    if not os.path.exists(index_path):
         raise HTTPException(status_code=404, detail="index.html not found")
    return FileResponse(index_path, media_type='text/html')

# --- User Endpoints ---
@app.post("/users/", response_model=schemas.User, tags=["Users"])
def create_user_endpoint(user: schemas.UserCreate, db: Session = Depends(get_db)):
    db_user = crud.get_user_by_email(db, email=user.email)
    if db_user:
        raise HTTPException(status_code=400, detail="Email already registered")
    return crud.create_user(db=db, user=user)

# --- User Profile Endpoints (Assuming user_id=1 for PoC) ---
@app.post("/users/{user_id}/profile/", response_model=schemas.UserProfile, tags=["User Profile"])
def create_or_update_profile_endpoint(user_id: int, profile: schemas.UserProfileCreate, db: Session = Depends(get_db)):
    profile_json_str = crud.create_or_update_user_profile(db=db, user_id=user_id, profile=profile)
    if profile_json_str is None:
        raise HTTPException(status_code=404, detail="User not found")

    # Convert stored JSON string back to dict for the response model
    profile_data = json.loads(profile_json_str)
    # We need owner_email for the response model. Fetch the user again.
    user = db.query(models.User).filter(models.User.id == user_id).first()
    if not user: # Should not happen if create_or_update was successful, but check anyway
         raise HTTPException(status_code=404, detail="User not found after profile update")

    return schemas.UserProfile(id=user_id, owner_email=user.email, profile_data=profile_data)

@app.get("/users/{user_id}/profile/", response_model=schemas.UserProfile, tags=["User Profile"])
def get_profile_endpoint(user_id: int, db: Session = Depends(get_db)):
    # PoC: Hardcode user_id check or assume user_id=1 if no auth
    if user_id != 1:
         raise HTTPException(status_code=403, detail="Operation not permitted for this user")

    profile_json_str = crud.get_user_profile(db=db, user_id=user_id)
    if profile_json_str is None:
        # Check if user exists but profile is just null
        user = db.query(models.User).filter(models.User.id == user_id).first()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        else:
             # User exists, but no profile set yet. Return default/empty or 404?
             # Let's return 404 for profile not found for consistency.
             raise HTTPException(status_code=404, detail="User profile not found")

    profile_data = json.loads(profile_json_str)
    user = db.query(models.User).filter(models.User.id == user_id).first() # Fetch user for email
    if not user: # Should not happen based on above logic, but defensive check
        raise HTTPException(status_code=404, detail="User not found when retrieving email for profile")

    return schemas.UserProfile(id=user_id, owner_email=user.email, profile_data=profile_data)

# --- Job Endpoints (Assuming user_id=1 for PoC) ---
@app.post("/users/{user_id}/jobs/", response_model=schemas.Job, tags=["Jobs"])
def create_job_endpoint(user_id: int, job: schemas.JobCreate, db: Session = Depends(get_db)):
    # Optional: Check if user exists before creating job
    user = db.query(models.User).filter(models.User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return crud.create_job(db=db, job=job, user_id=user_id)

@app.get("/users/{user_id}/jobs/", response_model=List[schemas.Job], tags=["Jobs"])
def get_jobs_endpoint(user_id: int, db: Session = Depends(get_db)):
     # PoC: Hardcode user_id check or assume user_id=1 if no auth
    if user_id != 1:
         raise HTTPException(status_code=403, detail="Operation not permitted for this user")
    jobs = crud.get_jobs_for_user(db=db, user_id=user_id)
    return jobs

# --- Pydantic Models ---
class TailoringRequest(BaseModel):
    job_description: str

class TailoringResponse(BaseModel):
    suggestions: str

# --- Job Ranking and Tailoring Endpoints ---
from pydantic import BaseModel
from typing import Optional

class JobRankResponse(BaseModel):
    score: Optional[float] = None
    explanation: Optional[str] = None

@app.post("/jobs/{job_id}/rank", response_model=JobRankResponse, tags=["LLM Features"])
async def rank_job_endpoint(job_id: int, db: Session = Depends(get_db)):
    """Triggers LLM ranking for a specific job based on user 1's profile."""
    # Assume user_id=1 for PoC
    user_id = 1
    score, explanation = await logic.rank_job_with_llm(db=db, job_id=job_id, user_id=user_id)
    if score is None or explanation is None:
        raise HTTPException(status_code=500, detail="Failed to rank job using LLM")
    
    # Explicitly commit the transaction to ensure changes are persisted
    db.commit()
    
    return {"score": score, "explanation": explanation}

class ResumeTailoringRequest(BaseModel):
    job_description: str
    profile_snippet: str

class ResumeTailoringResponse(BaseModel):
    suggestions: Optional[str] = None

@app.post("/resume/suggest_tailoring", response_model=ResumeTailoringResponse, tags=["LLM Features"])
async def suggest_tailoring_endpoint(request: ResumeTailoringRequest):
    """Generates resume tailoring suggestions based on job description and profile snippet."""
    suggestions = await logic.suggest_resume_tailoring(
        job_description=request.job_description,
        profile_snippet=request.profile_snippet
    )
    if suggestions is None:
        raise HTTPException(status_code=500, detail="Failed to generate resume tailoring suggestions using LLM")
    return {"suggestions": suggestions}

# --- Autofill Mapping Endpoint ---
@app.post("/autofill/map_poc", response_model=Dict[str, str], tags=["Autofill"])
async def map_autofill_fields_poc(
    form_fields: List[schemas.FormFieldInfo],
    user_id: int = 1,  # Default to user 1 for PoC, but allow override via query param
    db: Session = Depends(get_db)
):
    """POC endpoint to map user profile data to given form fields using LLM.
    
    First checks if the user profile exists before attempting to use LLM.
    """
    # First, check if this user has a profile
    profile_json_str = crud.get_user_profile(db=db, user_id=user_id)
    if profile_json_str is None:
        raise HTTPException(status_code=404, detail=f"Profile not found for user {user_id}")
        
    try:
        mapping = await logic.map_form_fields_with_llm(
            db=db,
            user_id=user_id,
            form_fields=form_fields
        )
        return mapping
    except Exception as e:
        # Catch potential errors during LLM call or processing
        # Log the error for debugging
        # logger.error(f"Error during autofill mapping: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to map autofill fields: {e}")

# --- New Endpoint ---
@app.post("/users/{user_id}/jobs/tailor", response_model=TailoringResponse)
async def get_tailoring_suggestions_endpoint(
    request_data: TailoringRequest,
    user_id: int = Path(..., title="The ID of the user to get suggestions for"),
    db: Session = Depends(get_db)
):
    """
    Generates tailoring suggestions for a given job description based on the user's profile.
    """
    # 1. Fetch user profile
    db_profile = crud.get_user_profile(db, user_id=user_id)
    if db_profile is None:
        raise HTTPException(status_code=404, detail=f"User profile not found for user_id {user_id}")

    # 2. Call logic function to get suggestions
    try:
        suggestions = await logic.get_tailoring_suggestions(
            profile_text=db_profile,
            job_description=request_data.job_description
        )
        return TailoringResponse(suggestions=suggestions)
    except Exception as e:
        # Basic error handling for LLM or other issues
        print(f"Error generating tailoring suggestions: {e}") # Log the error server-side
        raise HTTPException(status_code=500, detail="Failed to generate tailoring suggestions.")

# --- Dependency ---
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# --- Main execution --- (for running with uvicorn)
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

================
File: models.py
================
from sqlalchemy import Column, Integer, String, Float, ForeignKey
from sqlalchemy.orm import relationship

from database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    email = Column(String, unique=True, index=True)
    profile_json = Column(String) # Store profile as JSON string for PoC

    jobs = relationship("Job", back_populates="owner")

class Job(Base):
    __tablename__ = "jobs"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    title = Column(String)
    company = Column(String)
    description_text = Column(String)
    ranking_score = Column(Float, nullable=True)
    ranking_explanation = Column(String, nullable=True)

    owner = relationship("User", back_populates="jobs")

================
File: pytest.ini
================
[pytest]
asyncio_mode = auto

# Environment variables for testing
env =
    # Use a different database file for tests
    DATABASE_URL=sqlite:///./test_job_assistant_poc.db
    # Indicate we are running tests (optional, can be useful)
    TESTING=True

================
File: requirements.txt
================
fastapi
uvicorn[standard]
sqlalchemy
pydantic
python-dotenv
python-multipart # For form data/file uploads
google-generativeai

# Testing
pytest
httpx
pytest-asyncio
pytest-env

================
File: schemas.py
================
from pydantic import BaseModel, ConfigDict
from typing import Optional

# --- User Profile Schemas ---
class UserProfileBase(BaseModel):
    profile_data: dict # Simple dict for PoC profile

class UserProfileCreate(UserProfileBase):
    pass

class UserProfile(UserProfileBase):
    id: int
    owner_email: str

    model_config = ConfigDict(from_attributes=True)


# --- User Schemas ---
class UserBase(BaseModel):
    email: str

class UserCreate(UserBase):
    pass

class User(UserBase):
    id: int
    profile: UserProfile | None = None # Assuming profile is optional or loaded later

    model_config = ConfigDict(from_attributes=True)


# --- Form Field Info (from Frontend/Tampermonkey) ---
class FormFieldInfo(BaseModel):
    field_id: str         # Unique ID generated by the frontend script
    label: Optional[str] = None   # Text from associated <label> tag
    type: Optional[str] = None    # Input type (e.g., 'text', 'email', 'tel')
    name: Optional[str] = None    # Input 'name' attribute
    placeholder: Optional[str] = None # Input 'placeholder' attribute


# --- Job Schemas ---
class JobBase(BaseModel):
    title: str
    company: str
    description_text: str

class JobCreate(JobBase):
    pass

class Job(JobBase):
    id: int
    user_id: int
    ranking_score: float | None = None
    ranking_explanation: str | None = None

    model_config = ConfigDict(from_attributes=True)

================
File: tampermonkey_script.js
================


================
File: test_form.html
================


================
File: test_main.py
================
import pytest
from fastapi.testclient import TestClient
from sqlalchemy.orm import Session
import json

# Import your app and models/schemas
from main import app # Assuming test_client uses this
import crud
import models
import schemas
import logic # Needed for monkeypatching call_gemini

# --- Test Data --- 
TEST_USER = {"email": "test@example.com"}
TEST_PROFILE = {
    "profile_data": {
        "contact": {
            "firstName": "Test",
            "lastName": "User",
            "email": "test@example.com",
            "phone": "555-1234"
        },
        "summary": "A testing profile.",
        "skills": ["pytest", "fastapi", "mocking"],
        "experience": [
            {"company": "TestCorp", "title": "Tester", "years": 1}
        ]
    }
}
TEST_JOB = {
    "title": "Test Engineer",
    "company": "TestCorp",
    "description_text": "Need someone to write tests. FastAPI knowledge a plus."
}

# --- Helper to create user/profile/job --- 
def setup_test_data(db_session: Session) -> int:
    """Creates a standard user, profile, and job for testing LLM features. Returns job ID."""
    user = crud.create_user(db_session, schemas.UserCreate(**TEST_USER))
    crud.create_or_update_user_profile(db_session, user.id, schemas.UserProfileCreate(**TEST_PROFILE))
    job = crud.create_job(db_session, schemas.JobCreate(**TEST_JOB), user.id)
    # db_session.commit() # Let fixture handle commit
    return job.id

# --- Fixture to create user, profile, and job directly in DB for tests --- 
@pytest.fixture(scope="function")
def setup_data_fixture(db_session: Session, request):
    """Fixture to create user, profile, and job directly in DB for tests.
    Uses the test function name to create a unique email for each test.
    """
    # Create a unique email based on the test function name
    test_name = request.function.__name__
    user_data = TEST_USER.copy()
    user_data["email"] = f"{test_name}@example.com"
    
    # Create user, profile and job with the unique email
    user = crud.create_user(db_session, schemas.UserCreate(**user_data))
    
    # Use the same email in profile data
    profile_data = TEST_PROFILE.copy()
    profile_data["profile_data"] = dict(TEST_PROFILE["profile_data"])
    profile_data["profile_data"]["contact"] = dict(TEST_PROFILE["profile_data"]["contact"])
    profile_data["profile_data"]["contact"]["email"] = user_data["email"]
    
    crud.create_or_update_user_profile(db_session, user.id, schemas.UserProfileCreate(**profile_data))
    job = crud.create_job(db_session, schemas.JobCreate(**TEST_JOB), user.id)
    db_session.commit() # Commit prerequisites before test runs
    return {"user_id": user.id, "job_id": job.id, "email": user_data["email"]}

# --- Test Basic CRUD (Prerequisites) --- 

def test_create_user(test_client: TestClient):
    response = test_client.post("/users/", json=TEST_USER)
    assert response.status_code == 200
    data = response.json()
    assert data["email"] == TEST_USER["email"]
    assert "id" in data

def test_create_profile(test_client: TestClient): 
    user_id = 1

    # 2. Create profile via endpoint
    response = test_client.post(f"/users/{user_id}/profile/", json=TEST_PROFILE)
    assert response.status_code == 200
    data = response.json()
    # Assert based on schemas.UserProfile
    # Cannot easily assert owner_email without fetching user 1 first
    # assert data["owner_email"] == "test@example.com" # Expected if user 1 is TEST_USER
    assert data["id"] == user_id
    assert data["profile_data"] == TEST_PROFILE["profile_data"]

def test_create_job(test_client: TestClient): 
    user_id = 1

    # 2. Create job via endpoint
    response = test_client.post(f"/users/{user_id}/jobs/", json=TEST_JOB)
    assert response.status_code == 200
    # Assert based on schemas.Job
    data = response.json()
    assert data["title"] == TEST_JOB["title"]
    assert data["company"] == TEST_JOB["company"]
    assert data["description_text"] == TEST_JOB["description_text"]
    assert data["user_id"] == user_id
    assert "id" in data # Check job ID is present

# --- Test LLM Feature Endpoints --- 

# === Job Ranking ===
def test_rank_job_success(test_client: TestClient, setup_data_fixture, db_session, monkeypatch):
    # Use the fixture to get prerequisite data
    job_id = setup_data_fixture["job_id"]
    user_id = setup_data_fixture["user_id"] # Need user ID if endpoint uses it

    # Mock the LLM call
    mock_response = "Score: 7.5\nExplanation: Decent match based on skills."
    async def mock_call_gemini(*args, **kwargs):
        return mock_response
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    # Call the endpoint
    response = test_client.post(f"/jobs/{job_id}/rank")

    # Assertions
    assert response.status_code == 200
    data = response.json()
    assert data["score"] == 7.5
    assert data["explanation"] == "Decent match based on skills."

    # Verify DB update - explicitly refresh the db_session to see changes made by API call
    db_session.expire_all()  # Clear cached objects so we get fresh data
    
    # Query the job again to get the updated values
    db_job = crud.get_job(db_session, job_id=job_id, user_id=user_id)
    assert db_job is not None
    assert db_job.ranking_score == 7.5
    assert db_job.ranking_explanation == "Decent match based on skills."

def test_rank_job_llm_failure(test_client: TestClient, setup_data_fixture, db_session, monkeypatch):
    job_id = setup_data_fixture["job_id"]
    user_id = setup_data_fixture["user_id"]

    # Mock the LLM call to return None (simulating failure)
    async def mock_call_gemini(*args, **kwargs):
        return None
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    # Call the endpoint
    response = test_client.post(f"/jobs/{job_id}/rank")

    # Assertions
    assert response.status_code == 500
    # Check if the ranking was NOT updated in the db
    db_job = crud.get_job(db_session, job_id=job_id, user_id=user_id)
    db_session.refresh(db_job) # Ensure we get latest state
    assert db_job is not None
    assert db_job.ranking_score is None
    assert db_job.ranking_explanation is None

def test_rank_job_parse_failure(test_client: TestClient, setup_data_fixture, db_session, monkeypatch):
    job_id = setup_data_fixture["job_id"]
    user_id = setup_data_fixture["user_id"]

    # Mock the LLM call to return malformed text
    async def mock_call_gemini(*args, **kwargs):
        return "Score: ?? Explanation: ??"
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    response = test_client.post(f"/jobs/{job_id}/rank")
    assert response.status_code == 500 # Should fail during parsing in logic.py
    # Check if the ranking was NOT updated in the db
    db_job = crud.get_job(db_session, job_id=job_id, user_id=user_id)
    db_session.refresh(db_job)
    assert db_job is not None
    assert db_job.ranking_score is None
    assert db_job.ranking_explanation is None

# === Resume Tailoring ===
def test_suggest_tailoring_success(test_client: TestClient, monkeypatch):
    # Mock the LLM call
    mock_suggestions = "- Suggestion 1\n- Suggestion 2"
    async def mock_call_gemini(*args, **kwargs):
        return mock_suggestions
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    request_data = {
        "job_description": "Job desc...",
        "profile_snippet": "Profile snippet..."
    }
    response = test_client.post("/resume/suggest_tailoring", json=request_data)

    assert response.status_code == 200
    data = response.json()
    assert data["suggestions"] == mock_suggestions

def test_suggest_tailoring_llm_failure(test_client: TestClient, monkeypatch):
     # Mock the LLM call to return None
    async def mock_call_gemini(*args, **kwargs):
        return None
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    request_data = {
        "job_description": "Job desc...",
        "profile_snippet": "Profile snippet..."
    }
    response = test_client.post("/resume/suggest_tailoring", json=request_data)

    assert response.status_code == 500
    assert "Failed to generate resume tailoring suggestions" in response.text

# === Autofill Mapping ===
def test_autofill_map_success(test_client: TestClient, setup_data_fixture, db_session, monkeypatch):
    # Use the fixture which creates user 1 with profile
    user_id = setup_data_fixture["user_id"]

    form_fields_request = [
        {"field_id": "field_fname", "label": "First Name"},
        {"field_id": "field_email", "label": "Email"},
        {"field_id": "field_company", "label": "Company"},
        {"field_id": "field_skill_1", "label": "Skill 1"},
        {"field_id": "field_nonexistent", "label": "Something Else"},
        {"field_id": "field_badkey", "label": "Bad Key Test"}
    ]

    # Mock the LLM call (JSON mode)
    mock_key_mapping = {
        "field_fname": "contact.firstName",
        "field_email": "contact.email",
        "field_company": "experience.0.company",
        "field_skill_1": "skills.0",
        "field_nonexistent": "does.not.exist", # Test key not found
        "field_badkey": 123 # Test invalid key type from LLM
    }
    async def mock_call_gemini(*args, **kwargs):
        # Ensure expect_json=True was passed
        assert kwargs.get('expect_json') is True
        return mock_key_mapping
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    response = test_client.post(f"/autofill/map_poc?user_id={user_id}", json=form_fields_request)

    # Assertions
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, dict)
    # Check correct values are mapped
    assert data["field_fname"] == "Test" # From TEST_PROFILE -> contact.firstName
    assert data["field_email"] == setup_data_fixture["email"] # From unique email in fixture
    assert data["field_company"] == "TestCorp" # From TEST_PROFILE -> experience.0.company
    assert data["field_skill_1"] == "pytest" # From TEST_PROFILE -> skills.0
    # Check keys that shouldn't map are excluded
    assert "field_nonexistent" not in data
    assert "field_badkey" not in data

def test_autofill_map_llm_failure_json(test_client: TestClient, setup_data_fixture, db_session, monkeypatch):
    # Use the fixture which creates user 1 with profile
    user_id = setup_data_fixture["user_id"]

    # Mock LLM returning None
    async def mock_call_gemini(*args, **kwargs):
        assert kwargs.get('expect_json') is True
        return None
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    form_fields_request = [{"field_id": "f1", "label": "L1"}]
    response = test_client.post(f"/autofill/map_poc?user_id={user_id}", json=form_fields_request)

    assert response.status_code == 200 # Endpoint currently returns {} on LLM failure
    assert response.json() == {}

def test_autofill_map_llm_invalid_json(test_client: TestClient, setup_data_fixture, db_session, monkeypatch):
    # Use the fixture which creates user 1 with profile
    user_id = setup_data_fixture["user_id"]

    # Mock LLM returning non-dict
    async def mock_call_gemini(*args, **kwargs):
        assert kwargs.get('expect_json') is True
        return "this is not json"
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    form_fields_request = [{"field_id": "f1", "label": "L1"}]
    response = test_client.post(f"/autofill/map_poc?user_id={user_id}", json=form_fields_request)

    assert response.status_code == 200 # Endpoint currently returns {} on LLM failure
    assert response.json() == {}

def test_autofill_map_no_profile(test_client: TestClient, db_session: Session, monkeypatch):
    # Create user BUT NO PROFILE
    # Use a different email to avoid conflict with fixture
    user = crud.create_user(db_session, schemas.UserCreate(email="no_profile@example.com"))
    db_session.commit()
    user_id = user.id

    # Mock should not even be called if profile check works
    async def mock_call_gemini(*args, **kwargs):
        pytest.fail("LLM should not be called if profile is missing")
    monkeypatch.setattr(logic, "call_gemini", mock_call_gemini)

    form_fields_request = [{"field_id": "f1", "label": "L1"}]

    # Call the endpoint for the user without a profile
    response = test_client.post(f"/autofill/map_poc?user_id={user_id}", json=form_fields_request)

    assert response.status_code == 404 # Endpoint should return 404 if profile missing
    assert "Profile not found for user" in response.json()["detail"] # Check detail message
